{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e836616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import os\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79901933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "710529eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  1123\n"
     ]
    }
   ],
   "source": [
    "seed = 1123\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "print(\"Random Seed: \", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99756f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels = 1, num_hiddens = 128, hidden_dim=64, z=10):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.z = z\n",
    "\n",
    "        self._conv_0 = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=num_hiddens//4,\n",
    "                                 kernel_size=4,\n",
    "                                 stride=2, padding=1)\n",
    "\n",
    "        self._conv_1 = nn.Conv2d(in_channels=num_hiddens//4,\n",
    "                                 out_channels=num_hiddens//4,\n",
    "                                 kernel_size=4,\n",
    "                                 stride=2, padding=1)\n",
    "\n",
    "\n",
    "        self._conv_2 = nn.Conv2d(in_channels=num_hiddens//4,\n",
    "                                 out_channels=num_hiddens//2,\n",
    "                                 kernel_size=4,\n",
    "                                 stride=2, padding=1)\n",
    "\n",
    "\n",
    "        self._conv_3 = nn.Conv2d(in_channels=num_hiddens//2,\n",
    "                                 out_channels=num_hiddens//2,\n",
    "                                 kernel_size=4,\n",
    "                                 stride=2, padding=1)\n",
    "\n",
    "        self._fc1 = nn.Linear(1024, hidden_dim)\n",
    "\n",
    "        self.fc_mean = nn.Linear(hidden_dim, self.z)\n",
    "        #self.fc_logvar = nn.Linear(hidden_dim, self.z)\n",
    "        #self.fc_theta = nn.Linear(hidden_dim, self.z)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        x1 = F.leaky_relu(self._conv_0(inputs), 0.2, inplace=True)\n",
    "\n",
    "        x2 = self._conv_1(x1)\n",
    "        x2 = F.leaky_relu(x2, 0.2, inplace=True)\n",
    "\n",
    "        x3 = self._conv_2(x2)\n",
    "        x3 = F.leaky_relu(x3, 0.2, inplace=True)\n",
    "\n",
    "        x4 = self._conv_3(x3)\n",
    "        x4 = F.leaky_relu(x4, 0.2, inplace=True)\n",
    "\n",
    "        x5 = self._fc1(x4.view(-1, self.num_hiddens*8))\n",
    "        x5 = F.leaky_relu(x5, 0.2, inplace=True)\n",
    "\n",
    "        mean = self.fc_mean(x5)\n",
    "        #logvar = self.fc_logvar(x5)\n",
    "        return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2e43281",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim = 10, hidden_dim = 256, num_hiddens = 128, out_channels=1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.num_hiddens = num_hiddens\n",
    "\n",
    "        self._fc1 = nn.Linear(latent_dim, hidden_dim//2)\n",
    "\n",
    "        self._fc2 = nn.Linear(hidden_dim//2, 1024)\n",
    "\n",
    "        # the exact inverse of the encoder\n",
    "        self._tconv2 = nn.ConvTranspose2d(num_hiddens//2, num_hiddens//2, kernel_size=4, stride=2, padding =1)\n",
    "\n",
    "        self._tconv3 = nn.ConvTranspose2d(in_channels=num_hiddens//2,\n",
    "                                                out_channels=num_hiddens//4,\n",
    "                                                kernel_size=4,\n",
    "                                                stride=2, padding=1)\n",
    "\n",
    "        self._tconv4 = nn.ConvTranspose2d(in_channels=num_hiddens//4,\n",
    "                                                out_channels=num_hiddens//4,\n",
    "                                                kernel_size=4,\n",
    "                                                stride=2, padding=1)\n",
    "\n",
    "\n",
    "        self._tconv5 = nn.ConvTranspose2d(in_channels=num_hiddens//4,\n",
    "                                                out_channels=out_channels,\n",
    "                                                kernel_size=4,\n",
    "                                                stride=2, padding=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self._fc1(inputs)\n",
    "        x = F.leaky_relu(x, 0.2, inplace=True)\n",
    "\n",
    "        x = self._fc2(x)\n",
    "        x = F.leaky_relu(x, 0.2, inplace=True)\n",
    "\n",
    "        x = self._tconv2(x.view(-1,64,4,4))\n",
    "        x = F.leaky_relu(x, 0.2, inplace=True)\n",
    "\n",
    "        x = self._tconv3(x)\n",
    "        x = F.leaky_relu(x, 0.2, inplace=True)\n",
    "\n",
    "        x = self._tconv4(x)\n",
    "        x = F.leaky_relu(x, 0.2, inplace=True)\n",
    "\n",
    "        return torch.sigmoid(self._tconv5(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "452a7d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Discretization bottleneck part of the VQ-VAE.\n",
    "    Inputs:\n",
    "    - n_e : number of embeddings\n",
    "    - e_dim : dimension of embedding\n",
    "    - beta : commitment cost used in loss term, beta * ||z_e(x)-sg[e]||^2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_e, e_dim, beta):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.n_e = n_e\n",
    "        self.e_dim = e_dim\n",
    "        self.beta = beta\n",
    "\n",
    "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
    "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Inputs the output of the encoder network z and maps it to a discrete\n",
    "        one-hot vector that is the index of the closest embedding vector e_j\n",
    "        z (continuous) -> z_q (discrete)\n",
    "        z.shape = (batch, channel, height, width)\n",
    "        quantization pipeline:\n",
    "            1. get encoder input (B,C,H,W)\n",
    "            2. flatten input to (B*H*W,C)\n",
    "        \"\"\"\n",
    "        # reshape z -> (batch, height, width, channel) and flatten\n",
    "        #z = z.contiguous()\n",
    "        #z_flattened = z.view(-1, self.e_dim)\n",
    "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
    "\n",
    "        d = torch.sum(z ** 2, dim=1, keepdim=True) + \\\n",
    "            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n",
    "            torch.matmul(z, self.embedding.weight.t())\n",
    "                # find closest encodings\n",
    "        min_encoding_indices = torch.argmin(d, dim=1).unsqueeze(1)\n",
    "        min_encodings = torch.zeros(min_encoding_indices.shape[0], self.n_e).to(device)\n",
    "        min_encodings.scatter_(1, min_encoding_indices, 1)\n",
    "\n",
    "        # get quantized latent vectors\n",
    "        z_q = torch.matmul(min_encodings, self.embedding.weight).view(z.shape)\n",
    "\n",
    "        # compute loss for embedding\n",
    "        loss = torch.mean((z_q.detach()-z)**2) + self.beta * torch.mean((z_q - z.detach()) ** 2)\n",
    "\n",
    "        # preserve gradients\n",
    "        z_q = z + (z_q - z).detach()\n",
    "\n",
    "        # perplexity\n",
    "        e_mean = torch.mean(min_encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(e_mean * torch.log(e_mean + 1e-10)))\n",
    "\n",
    "        # reshape back to match original input shape\n",
    "        #z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        return loss, z_q, perplexity, min_encodings, min_encoding_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60da5cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, img_size = (1,64,64), enc_hidden = 256, hidden_dim = 64, dis_hidden = 32, z = 5, beta=0.25, nembed = 10, dec_hidden = 256, use_cuda=False):\n",
    "\n",
    "        super(VAE, self).__init__()\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "        # Parameters\n",
    "        self.img_size = img_size\n",
    "        self.num_pixels = img_size[0] * img_size[1] * img_size[2]\n",
    "        self.z = z\n",
    "        self.nembed = nembed\n",
    "        self.channels = img_size[0]\n",
    "\n",
    "        # Define the encoder\n",
    "        self.encoder = Encoder(in_channels=self.channels, hidden_dim=hidden_dim, z=self.z)\n",
    "\n",
    "        # Define decoder\n",
    "        self.decoder = Decoder(self.z, out_channels=self.channels)\n",
    "\n",
    "        self.vector_quantization = nn.ModuleDict()\n",
    "\n",
    "        for i in range(self.z):\n",
    "            self.vector_quantization['dict{}'.format(i)] = VectorQuantizer(nembed, 1, beta)\n",
    "            \n",
    "    def forward(self, data):\n",
    "        mean = self.encode(data)\n",
    "\n",
    "        # vector quantization for each dimension\n",
    "        z, embedding_loss, perplexity = [], [], []\n",
    "        for i in range(self.z):\n",
    "            e_loss, z_q, p, _, _ = self.vector_quantization[i](mean[:,i])\n",
    "            z.append(z_q)\n",
    "            embedding_loss.append(e_loss)\n",
    "            perplexity.append(p)\n",
    "\n",
    "        print(z[0].shape)\n",
    "        exit()\n",
    "        sample = torch.cat(z,1)\n",
    "        recons = self.decode(sample)\n",
    "        return torch.tensor(embedding_loss), recons, torch.tensor(perplexity)\n",
    "\n",
    "    def encode(self,x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, latent_sample):\n",
    "        return self.decoder(latent_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afb340f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    batch_size = 10\n",
    "    img_size = (3,64,64)\n",
    "    z=1\n",
    "    model = VAE(img_size=img_size, z=z, nembed=10, use_cuda=use_cuda)\n",
    "\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "    checkpoint = torch.load('weights/protofactooldprotofrwr_fawr_weights_e400_z10_b1_g10_lr[0.0001, 0.0001, 0.0001]_bs128_s1123_r[20, 0, 20, 80, 0, False]_d3',map_location=torch.device('cpu'))\n",
    "    plots = ['samples', 'reconstruct-traverse']\n",
    "    loss_file = 'weights/protofactooldprotofrwr_fawr_losses_e400_z10_b1_g10_lr[0.0001, 0.0001, 0.0001]_bs128_s1123_r[20, 0, 20, 80, 0, False]_d3.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f986b94",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../3dshapes/3dshapes_data_c.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../3dshapes/3dshapes_data_c.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      2\u001b[0m test_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../3dshapes/3dshapes_labels_c.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      4\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mTensorDataset(test_data,test_labels)\n",
      "File \u001b[0;32m~/env/lib/python3.9/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../3dshapes/3dshapes_data_c.npy'"
     ]
    }
   ],
   "source": [
    "test_data = torch.from_numpy(np.load('../3dshapes/3dshapes_data_c.npy')).permute(0,3,1,2).float()\n",
    "test_labels = torch.from_numpy(np.load('../3dshapes/3dshapes_labels_c.npy')).float()\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(test_data,test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 10, shuffle=True, drop_last=True)\n",
    "print(len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bd0c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualizer():\n",
    "    def __init__(self, model, model_dir, loss_file, \n",
    "                 save_images=True,\n",
    "                 loss_of_interest=None,\n",
    "                 display_loss_per_dim=False,\n",
    "                 max_traversal=0.475,  # corresponds to ~2 for standard normal\n",
    "                 upsample_factor=1):\n",
    "        \"\"\"\n",
    "        Visualizer is used to generate images of samples, reconstructions,\n",
    "        latent traversals and so on of the trained model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : disvae.vae.VAE\n",
    "        dataset : str\n",
    "            Name of the dataset.\n",
    "        model_dir : str\n",
    "            The directory that the model is saved to and where the images will\n",
    "            be stored.\n",
    "        save_images : bool, optional\n",
    "            Whether to save images or return a tensor.\n",
    "        loss_of_interest : str, optional\n",
    "            The loss type (as saved in the log file) to order the latent dimensions by and display.\n",
    "        display_loss_per_dim : bool, optional\n",
    "            if the loss should be included as text next to the corresponding latent dimension images.\n",
    "        max_traversal: float, optional\n",
    "            The maximum displacement induced by a latent traversal. Symmetrical\n",
    "            traversals are assumed. If `m>=0.5` then uses absolute value traversal,\n",
    "            if `m<0.5` uses a percentage of the distribution (quantile).\n",
    "            E.g. for the prior the distribution is a standard normal so `m=0.45` c\n",
    "            orresponds to an absolute value of `1.645` because `2m=90%%` of a\n",
    "            standard normal is between `-1.645` and `1.645`. Note in the case\n",
    "            of the posterior, the distribution is not standard normal anymore.\n",
    "        upsample_factor : floar, optional\n",
    "            Scale factor to upsample the size of the tensor\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.device = next(self.model.parameters()).device\n",
    "        self.latent_dim = self.model.z\n",
    "        self.max_traversal = max_traversal\n",
    "        self.save_images = save_images\n",
    "        self.model_dir = model_dir \n",
    "        self.dataset = 'dsprites'\n",
    "        self.upsample_factor = upsample_factor\n",
    "        if loss_of_interest is not None:\n",
    "            # get this from your loss file\n",
    "            self.losses = read_loss_from_file(loss_file)\n",
    "\n",
    "    def _get_traversal_range(self, mean=0, std=1):\n",
    "        \"\"\"Return the corresponding traversal range in absolute terms.\"\"\"\n",
    "        max_traversal = self.max_traversal\n",
    "\n",
    "        if max_traversal < 0.5:\n",
    "            max_traversal = (1 - 2 * max_traversal) / 2  # from 0.45 to 0.05\n",
    "            max_traversal = stats.norm.ppf(max_traversal, loc=mean, scale=std)  # from 0.05 to -1.645\n",
    "\n",
    "        # symmetrical traversals\n",
    "        return (-1 * max_traversal, max_traversal)\n",
    "\n",
    "    def _traverse_line(self, idx, n_samples, data=None):\n",
    "        \"\"\"Return a (size, latent_size) latent sample, corresponding to a traversal\n",
    "        of a latent variable indicated by idx.\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Index of continuous dimension to traverse. If the continuous latent\n",
    "            vector is 10 dimensional and idx = 7, then the 7th dimension\n",
    "            will be traversed while all others are fixed.\n",
    "        n_samples : int\n",
    "            Number of samples to generate.\n",
    "        data : torch.Tensor or None, optional\n",
    "            Data to use for computing the posterior. Shape (N, C, H, W). If\n",
    "            `None` then use the mean of the prior (all zeros) for all other dimensions.\n",
    "        \"\"\"\n",
    "        if data is None:\n",
    "            # mean of prior for other dimensions\n",
    "            samples = torch.zeros(n_samples, self.latent_dim)\n",
    "            traversals = torch.linspace(*self._get_traversal_range(), steps=n_samples)\n",
    "\n",
    "        else:\n",
    "            if data.size(0) > 1:\n",
    "                raise ValueError(\"Every value should be sampled from the same posterior, but {} datapoints given.\".format(data.size(0)))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                post_mean, post_logvar = self.model.encode(data.to(self.device))\n",
    "                samples = self.model.reparameterize([post_mean, post_logvar])\n",
    "                samples = samples.cpu().repeat(n_samples, 1)\n",
    "                post_mean_idx = post_mean.cpu()[0, idx]\n",
    "                post_std_idx = torch.exp(post_logvar / 2).cpu()[0, idx]\n",
    "\n",
    "            # travers from the gaussian of the posterior in case quantile\n",
    "            traversals = torch.linspace(*self._get_traversal_range(mean=post_mean_idx,\n",
    "                                                                   std=post_std_idx),\n",
    "                                        steps=n_samples)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            samples[i, idx] = traversals[i]\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def _save_or_return(self, to_plot, size, filename, is_force_return=False):\n",
    "        \"\"\"Create plot and save or return it.\"\"\"\n",
    "        to_plot = F.interpolate(to_plot, scale_factor=self.upsample_factor)\n",
    "\n",
    "        if size[0] * size[1] != to_plot.shape[0]:\n",
    "            raise ValueError(\"Wrong size {} for datashape {}\".format(size, to_plot.shape))\n",
    "\n",
    "        # `nrow` is number of images PER row => number of col\n",
    "        kwargs = dict(nrow=size[1], pad_value=1)\n",
    "        if self.save_images and not is_force_return:\n",
    "            filename = os.path.join(self.model_dir, filename)\n",
    "            save_image(to_plot, filename, **kwargs)\n",
    "        else:\n",
    "            return make_grid_img(to_plot, **kwargs)\n",
    "\n",
    "    def _decode_latents(self, latent_samples):\n",
    "        \"\"\"Decodes latent samples into images.\n",
    "        Parameters\n",
    "        ----------\n",
    "        latent_samples : torch.autograd.Variable\n",
    "            Samples from latent distribution. Shape (N, L) where L is dimension\n",
    "            of latent distribution.\n",
    "        \"\"\"\n",
    "        latent_samples = latent_samples.to(self.device)\n",
    "        return self.model.decode(latent_samples).cpu()\n",
    "\n",
    "    def generate_samples(self, size=(8, 8)):\n",
    "        \"\"\"Plot generated samples from the prior and decoding.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size : tuple of ints, optional\n",
    "            Size of the final grid.\n",
    "        \"\"\"\n",
    "        prior_samples = torch.randn(size[0] * size[1], self.latent_dim)\n",
    "        generated = self._decode_latents(prior_samples)\n",
    "        return self._save_or_return(generated.data, size, PLOT_NAMES[\"generate_samples\"])\n",
    "\n",
    "    def data_samples(self, data, size=(8, 8)):\n",
    "        \"\"\"Plot samples from the dataset\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : torch.Tensor\n",
    "            Data to be reconstructed. Shape (N, C, H, W)\n",
    "        size : tuple of ints, optional\n",
    "            Size of the final grid.\n",
    "        \"\"\"\n",
    "        data = data[:size[0] * size[1], ...]\n",
    "        return self._save_or_return(data, size, PLOT_NAMES[\"data_samples\"])\n",
    "\n",
    "    def reconstruct(self, data, size=(8, 8), is_original=True, is_force_return=False):\n",
    "        \"\"\"Generate reconstructions of data through the model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : torch.Tensor\n",
    "            Data to be reconstructed. Shape (N, C, H, W)\n",
    "        size : tuple of ints, optional\n",
    "            Size of grid on which reconstructions will be plotted. The number\n",
    "            of rows should be even when `is_original`, so that upper\n",
    "            half contains true data and bottom half contains reconstructions.contains\n",
    "        is_original : bool, optional\n",
    "            Whether to exclude the original plots.\n",
    "        is_force_return : bool, optional\n",
    "            Force returning instead of saving the image.\n",
    "        \"\"\"\n",
    "        if is_original:\n",
    "            if size[0] % 2 != 0:\n",
    "                raise ValueError(\"Should be even number of rows when showing originals not {}\".format(size[0]))\n",
    "            n_samples = size[0] // 2 * size[1]\n",
    "        else:\n",
    "            n_samples = size[0] * size[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            originals = data.to(self.device)[:n_samples, ...]\n",
    "            recs,_ = self.model(originals)\n",
    "\n",
    "        originals = originals.cpu()\n",
    "        recs = recs.view(-1, *self.model.img_size).cpu()\n",
    "\n",
    "        to_plot = torch.cat([originals, recs]) if is_original else recs\n",
    "        return self._save_or_return(to_plot, size, PLOT_NAMES[\"reconstruct\"],\n",
    "                                    is_force_return=is_force_return)\n",
    "\n",
    "    def traversals(self,\n",
    "                   data=None,\n",
    "                   is_reorder_latents=False,\n",
    "                   n_per_latent=8,\n",
    "                   n_latents=None,\n",
    "                   is_force_return=False):\n",
    "        \"\"\"Plot traverse through all latent dimensions (prior or posterior) one\n",
    "        by one and plots a grid of images where each row corresponds to a latent\n",
    "        traversal of one latent dimension.\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : bool, optional\n",
    "            Data to use for computing the latent posterior. If `None` traverses\n",
    "            the prior.\n",
    "        n_per_latent : int, optional\n",
    "            The number of points to include in the traversal of a latent dimension.\n",
    "            I.e. number of columns.\n",
    "        n_latents : int, optional\n",
    "            The number of latent dimensions to display. I.e. number of rows. If `None`\n",
    "            uses all latents.\n",
    "        is_reorder_latents : bool, optional\n",
    "            If the latent dimensions should be reordered or not\n",
    "        is_force_return : bool, optional\n",
    "            Force returning instead of saving the image.\n",
    "        \"\"\"\n",
    "        n_latents = n_latents if n_latents is not None else self.model.latent_dim\n",
    "        latent_samples = [self._traverse_line(dim, n_per_latent, data=data)\n",
    "                          for dim in range(self.latent_dim)]\n",
    "        \n",
    "        # change one line here\n",
    "        lats = torch.cat(latent_samples, dim=0) #*torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]).view(1, 10)\n",
    "        \n",
    "        decoded_traversal = self._decode_latents(lats)\n",
    "\n",
    "        if is_reorder_latents:\n",
    "            n_images, *other_shape = decoded_traversal.size()\n",
    "            n_rows = n_images // n_per_latent\n",
    "            decoded_traversal = decoded_traversal.reshape(n_rows, n_per_latent, *other_shape)\n",
    "            decoded_traversal = sort_list_by_other(decoded_traversal, self.losses)\n",
    "            decoded_traversal = torch.stack(decoded_traversal, dim=0)\n",
    "            decoded_traversal = decoded_traversal.reshape(n_images, *other_shape)\n",
    "            \n",
    "        decoded_traversal = decoded_traversal[range(n_per_latent * n_latents), ...]\n",
    "\n",
    "        size = (n_latents, n_per_latent)\n",
    "        sampling_type = \"prior\" if data is None else \"posterior\"\n",
    "        filename = \"{}_{}\".format(sampling_type, PLOT_NAMES[\"traversals\"])\n",
    "\n",
    "        return self._save_or_return(decoded_traversal.data, size, filename,\n",
    "                                    is_force_return=is_force_return)\n",
    "\n",
    "    def reconstruct_traverse(self, data,\n",
    "                             is_posterior=True,\n",
    "                             n_per_latent=10,\n",
    "                             n_latents=None,\n",
    "                             is_show_text=True):\n",
    "        \"\"\"\n",
    "        Creates a figure whith first row for original images, second are\n",
    "        reconstructions, rest are traversals (prior or posterior) of the latent\n",
    "        dimensions.\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : torch.Tensor\n",
    "            Data to be reconstructed. Shape (N, C, H, W)\n",
    "        n_per_latent : int, optional\n",
    "            The number of points to include in the traversal of a latent dimension.\n",
    "            I.e. number of columns.\n",
    "        n_latents : int, optional\n",
    "            The number of latent dimensions to display. I.e. number of rows. If `None`\n",
    "            uses all latents.\n",
    "        is_posterior : bool, optional\n",
    "            Whether to sample from the posterior.\n",
    "        is_show_text : bool, optional\n",
    "            Whether the KL values next to the traversal rows.\n",
    "        \"\"\"\n",
    "        n_latents = n_latents if n_latents is not None else self.model.latent_dim\n",
    "\n",
    "        reconstructions = self.reconstruct(data[:2 * n_per_latent, ...],\n",
    "                                           size=(2, n_per_latent),\n",
    "                                           is_force_return=True)\n",
    "        traversals = self.traversals(data=data[3:4, ...] if is_posterior else None,\n",
    "                                     is_reorder_latents=False,\n",
    "                                     n_per_latent=n_per_latent,\n",
    "                                     n_latents=n_latents,\n",
    "                                     is_force_return=True)\n",
    "\n",
    "        concatenated = np.concatenate((reconstructions, traversals), axis=0)\n",
    "        concatenated = Image.fromarray(concatenated)\n",
    "        concatenated.show()\n",
    "\n",
    "        if is_show_text:\n",
    "            losses = sorted(self.losses, reverse=True)[:n_latents]\n",
    "            labels = ['orig', 'recon'] + [\"KL={:.4f}\".format(l) for l in losses]\n",
    "            concatenated = add_labels(concatenated, labels)\n",
    "\n",
    "        filename = os.path.join(self.model_dir, PLOT_NAMES[\"reconstruct_traverse\"])\n",
    "        concatenated.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1ca68a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model, checkpoint):\n",
    "    model.load_state_dict(checkpoint['vqvae'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def make_grid_img(tensor, **kwargs):\n",
    "    \"\"\"Converts a tensor to a grid of images that can be read by imageio.\n",
    "    Notes\n",
    "    -----\n",
    "    * from in https://github.com/pytorch/vision/blob/master/torchvision/utils.py\n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor (torch.Tensor or list): 4D mini-batch Tensor of shape (B x C x H x W)\n",
    "        or a list of images all of the same size.\n",
    "    kwargs:\n",
    "        Additional arguments to `make_grid_img`.\n",
    "    \"\"\"\n",
    "    grid = make_grid(tensor, **kwargs)\n",
    "    img_grid = grid.mul_(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0)\n",
    "    img_grid = img_grid.to('cpu', torch.uint8).numpy()\n",
    "    return img_grid\n",
    "\n",
    "\n",
    "# def get_samples\n",
    "def get_samples(dataset, num_samples, idcs=[]):\n",
    "    \"\"\" Generate a number of samples from the dataset.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : str\n",
    "        The name of the dataset.\n",
    "    num_samples : int, optional\n",
    "        The number of samples to load from the dataset\n",
    "    idcs : list of ints, optional\n",
    "        List of indices to of images to put at the begning of the samples.\n",
    "    \"\"\"\n",
    "    data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                  batch_size=1,\n",
    "                                  shuffle=idcs is None)\n",
    "\n",
    "    idcs += random.sample(range(len(data_loader.dataset)), num_samples - len(idcs))\n",
    "    samples = torch.stack([data_loader.dataset[i][0] for i in idcs], dim=0)\n",
    "    print(\"Selected idcs: {}\".format(idcs))\n",
    "\n",
    "    return samples\n",
    "\n",
    "def sort_list_by_other(to_sort, other, reverse=True):\n",
    "    \"\"\"Sort a list by an other.\"\"\"\n",
    "    return [el for _, el in sorted(zip(other, to_sort), reverse=reverse)]\n",
    "\n",
    "def read_loss_from_file(loss_file):\n",
    "    with open(loss_file) as f:\n",
    "        data = json.load(f)\n",
    "    kllist =[]\n",
    "    for key in list(data):\n",
    "        if 'kl_' in key:\n",
    "            if 'kl_z' not in key:\n",
    "                kllist.append(np.sum(data[key][-100:])/100)\n",
    "    return kllist\n",
    "\n",
    "def add_labels(input_image, labels):\n",
    "    \"\"\"Adds labels next to rows of an image.\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_image : image\n",
    "        The image to which to add the labels\n",
    "    labels : list\n",
    "        The list of labels to plot\n",
    "    \"\"\"\n",
    "    new_width = input_image.width + 100\n",
    "    new_size = (new_width, input_image.height)\n",
    "    new_img = Image.new(\"RGB\", new_size, color='white')\n",
    "    new_img.paste(input_image, (0, 0))\n",
    "    draw = ImageDraw.Draw(new_img)\n",
    "\n",
    "    for i, s in enumerate(labels):\n",
    "        draw.text(xy=(new_width - 100 + 0.005,\n",
    "                      int((i / len(labels) + 1 / (2 * len(labels))) * input_image.height)),\n",
    "                  text=s,\n",
    "                  fill=(0, 0, 0))\n",
    "\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d02e0b7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'vqvae'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m n_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#model = VAE(img_size=img_size, z1=2, z2=10, use_cuda=use_cuda)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3dshapes\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m viz \u001b[38;5;241m=\u001b[39m Visualizer(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     15\u001b[0m                  model_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m                  max_traversal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     17\u001b[0m                  loss_of_interest\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkl_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, loss_file\u001b[38;5;241m=\u001b[39mloss_file,\n\u001b[1;32m     18\u001b[0m                  upsample_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \n",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m, in \u001b[0;36mload_weights\u001b[0;34m(model, checkpoint)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_weights\u001b[39m(model, checkpoint):\n\u001b[0;32m----> 2\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvqvae\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[0;31mKeyError\u001b[0m: 'vqvae'"
     ]
    }
   ],
   "source": [
    "PLOT_NAMES = dict(generate_samples=\"samples.png\",\n",
    "                  data_samples=\"data_samples.png\",\n",
    "                  reconstruct=\"reconstruct.png\",\n",
    "                  traversals=\"traversals.png\",\n",
    "                  reconstruct_traverse=\"reconstruct_traverse.png\")\n",
    "n_rows = z\n",
    "#n_rows = model.latent_dim\n",
    "n_cols = 10\n",
    "\n",
    "#model = VAE(img_size=img_size, z1=2, z2=10, use_cuda=use_cuda)\n",
    "model = load_weights(model, checkpoint)\n",
    "dataset = '3dshapes'\n",
    "    \n",
    "viz = Visualizer(model=model,\n",
    "                 model_dir='',\n",
    "                 max_traversal=2,\n",
    "                 loss_of_interest='kl_loss', loss_file=loss_file,\n",
    "                 upsample_factor=1) \n",
    "size = (n_rows, n_cols)\n",
    "# same samples for all plots: sample max then take first `x`data  for all plots\n",
    "num_samples = n_cols * n_rows\n",
    "    \n",
    "samples = get_samples(test_dataset, num_samples)\n",
    "    \n",
    "if \"all\" in plots:\n",
    "    plots = [p for p in PLOT_TYPES if p != \"all\"]\n",
    "\n",
    "# mostly call it with reconstruct-traverse\n",
    "\n",
    "for plot_type in plots:\n",
    "    if plot_type == 'generate-samples':\n",
    "        viz.generate_samples(size=size)\n",
    "    elif plot_type == 'data-samples':\n",
    "        viz.data_samples(samples, size=size)\n",
    "    elif plot_type == \"reconstruct\":\n",
    "        viz.reconstruct(samples, size=size)\n",
    "    elif plot_type == \"reconstruct-traverse\":\n",
    "        viz.reconstruct_traverse(samples,\n",
    "                                 is_posterior=True,\n",
    "                                 n_latents=n_rows,\n",
    "                                 n_per_latent=n_cols,\n",
    "                                 is_show_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b44c96a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'latent_dist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m lat\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m16\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     mu \u001b[38;5;241m=\u001b[39m \u001b[43mlatent_dist\u001b[49m[\u001b[38;5;241m0\u001b[39m][i,lat]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      4\u001b[0m     variance \u001b[38;5;241m=\u001b[39m latent_dist[\u001b[38;5;241m1\u001b[39m][i,lat]\u001b[38;5;241m.\u001b[39mexp()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      5\u001b[0m     sigma \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(variance)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'latent_dist' is not defined"
     ]
    }
   ],
   "source": [
    "lat=0\n",
    "for i in range(16):\n",
    "    mu = latent_dist[0][i,lat].detach().numpy()\n",
    "    variance = latent_dist[1][i,lat].exp().detach().numpy()\n",
    "    sigma = math.sqrt(variance)\n",
    "    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "    plt.plot(x, stats.norm.pdf(x, mu, sigma))#, c = (labels[i,5].detach().numpy().tolist(),0.2,0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96c48e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images_grid(imgs_, num_images=64):\n",
    "    imgs_ = imgs_.numpy()\n",
    "    #ncols = int(np.ceil(num_images**0.5))\n",
    "    #nrows = int(np.ceil(num_images / ncols))\n",
    "    ncols=10\n",
    "    nrows=10\n",
    "    _, axes = plt.subplots(ncols, nrows, figsize=(nrows * 3, ncols * 3))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax_i, ax in enumerate(axes):\n",
    "        if ax_i < num_images:\n",
    "            ax.imshow(imgs_[ax_i], cmap='Greys_r', interpolation='nearest')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        else:\n",
    "            ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "935544b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_originals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m show_images_grid(\u001b[43mvalid_originals\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid_originals' is not defined"
     ]
    }
   ],
   "source": [
    "show_images_grid(valid_originals.permute(0,2,3,1).cpu().data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cf58f7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_reconstructions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m show_images_grid(\u001b[43mvalid_reconstructions\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid_reconstructions' is not defined"
     ]
    }
   ],
   "source": [
    "show_images_grid(valid_reconstructions.permute(0,2,3,1).cpu().data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5463b3fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(data):\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m      6\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(data[key][:])\n\u001b[1;32m      7\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtitle(key)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "with open(loss_file) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for key in list(data):\n",
    "    plt.figure(figsize=(4,3))\n",
    "    plt.plot(data[key][:])\n",
    "    plt.title(key)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c307c46e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
